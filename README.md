# Overview


<p align="center">

  <img src="image/title.png" width="800px"/>



We present the Unified Safety Benchmark (USB) for MLLMs, which provides 4 distinct modality combinations for each of the 61 risk sub-categories, covering both English and Chinese across both vulnerability and oversensitivity dimensions.

**USB-SafeBench's Features**

- **Comprehensive**:SB-SafeBench covers 4 different modality combinations across 3 major topics, 16 secondary topics and 61 fine-grained subtopics in detail.
- **High-quality**:USB-SafeBench undergoes a comprehensive and rigorous quality control process to ensure the quality of the dataset.
- **Adversarial Robust**:Samples in USB-SafeBench are aggressive in safety and should achieve high attack successful rates (ASR) against most mainstream VLMs.

---

**Topics and Subtopics**
- **3 Major Topics**:
- **16 Secondary Topics**:
- **31 Diverse Subtopics**:

**Modalities**

**Languages**
---

**USB-SafeBench serves as a valuable tool for**:
- Evaluating the safety of MLLM.

This benchmark is an important resource for developers and researchers working to improve the security and reliability of MLLM.

[//]: # (Please visit our [website]&#40;https://openstellarteam.github.io/ChineseSafetyQA/&#41;)

[//]: # (or check our [paper]&#40;https://arxiv.org/abs/2412.15265&#41; for more details.)

[//]: # (This is the evaluation repository for Chinese SafetyQA)

<p align="center">
  <img src="image/category_en.png" width="700px"/>
</p>


# LeaderBoard
## USB-Base Evaluate Result
| MLLMs | ASR | ARR | ASR Rank | ARR Rank | National Safety | | | | Public Safety | | | | Ethical Safety | | | | Total |
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
| | | | | | RIRT | SIRT | RIST | SIST | RIRT | SIRT | RIST | SIST | RIRT | SIRT | RIST | SIST | |
| Claude35-sonnet2 | 3.08% | 4.53% | 1 | 16 | 32.62% | 32.35% | 2.61% | 2.95% | 20.53% | 31.95% | 6.41% | 5.24% | 28.13% | 34.11% | 15.57% | 25.82% | 15.57% |
| Gemini-1.5-pro | 21.43% | 31.14% | 2 | 11 | 59.93% | 71.08% | 16.25% | 24.13% | 49.53% | 65.55% | 16.24% | 32.78% | 42.74% | 56.73% | 37.83% | 11.27% | 37.83% |
| GPT-4o | 16.60% | 24.90% | 3 | 9 | 65.53% | 72.02% | 13.02% | 12.45% | 52.71% | 74.04% | 22.99% | 28.35% | 60.23% | 72.44% | 40.93% | 6.81% | 40.93% |
| Gemini-2.0-flash | 25.75% | 39.02% | 4 | 5 | 81.59% | 78.50% | 16.75% | 21.95% | 64.70% | 76.54% | 23.38% | 35.81% | 64.07% | 74.60% | 46.51% | 5.43% | 46.51% |
| Qwen2.5-VL-72B | 30.64% | 38.49% | 5 | 1 | 75.27% | 88.24% | 25.47% | 26.82% | 67.86% | 80.71% | 25.31% | 34.48% | 60.16% | 68.85% | 48.80% | 1.43% | 48.80% |
| InternVL2-40B | 38.70% | 45.20% | 6 | 13 | 78.65% | 85.05% | 29.15% | 29.74% | 72.82% | 81.94% | 26.42% | 33.87% | 66.67% | 72.93% | 51.72% | 11.76% | 51.72% |
| Qwen2-VL-7B | 41.28% | 42.61% | 7 | 7 | 80.65% | 83.74% | 31.39% | 31.37% | 76.52% | 84.22% | 34.92% | 40.66% | 70.76% | 75.00% | 55.33% | 6.27% | 55.33% |
| GLM-4V-9B | 43.29% | 55.48% | 8 | 6 | 77.06% | 84.73% | 37.63% | 35.50% | 76.19% | 78.84% | 38.39% | 45.71% | 70.04% | 69.60% | 56.73% | 5.99% | 56.73% |
| Qwen-VL-max | 43.63% | 49.00% | 9 | 3 | 86.13% | 88.70% | 35.54% | 34.52% | 75.45% | 85.41% | 32.97% | 41.95% | 69.20% | 78.78% | 56.87% | 3.77% | 56.87% |
| Qwen2-VL-72B | 39.60% | 43.49% | 10 | 2 | 87.73% | 91.63% | 34.63% | 33.96% | 79.30% | 87.31% | 35.50% | 44.47% | 70.34% | 74.27% | 57.54% | 1.87% | 57.54% |
| InternVL2-8B | 58.05% | 57.79% | 11 | 14 | 87.36% | 87.25% | 44.64% | 40.50% | 79.13% | 82.88% | 39.16% | 39.48% | 70.09% | 72.19% | 59.73% | 11.97% | 59.73% |
| Qwen2.5-VL-7B | 70.37% | 75.86% | 12 | 4 | 86.69% | 94.09% | 59.52% | 59.15% | 79.35% | 88.15% | 51.93% | 61.62% | 71.88% | 75.73% | 69.76% | 4.73% | 69.76% |
| LLAVA-v1.5-13B | 63.39% | 69.18% | 13 | 12 | 90.97% | 94.53% | 62.45% | 62.25% | 88.28% | 88.77% | 51.90% | 61.97% | 81.01% | 76.41% | 72.72% | 11.39% | 72.72% |
| MiniCPM-V 2.6 | 78.60% | 76.98% | 14 | 8 | 88.13% | 86.21% | 73.75% | 70.18% | 76.77% | 85.78% | 62.55% | 69.05% | 68.97% | 72.02% | 73.86% | 6.43% | 73.86% |
| LLAVA-v1.5-7B | 80.48% | 81.31% | 15 | 10 | 87.27% | 87.19% | 82.76% | 80.87% | 87.53% | 85.79% | 77.46% | 81.15% | 84.65% | 80.55% | 83.07% | 8.54% | 83.07% |
| VILA-13B | 91.28% | 89.62% | 16 | 15 | 95.67% | 90.69% | 90.81% | 89.27% | 89.75% | 87.92% | 85.06% | 86.21% | 85.57% | 78.59% | 87.79% | 22.34% | 87.79% |
| VILA-7B | 92.28% | 85.91% | 17 | 17 | 94.95% | 91.67% | 89.05% | 87.53% | 91.48% | 90.22% | 89.78% | 88.93% | 90.58% | 84.14% | 89.35% | 32.51% | 89.35% |

## USB-Hard Evaluate Result
| MLLMs | National Safety | | | | Public Safety | | | | Ethical Safety | | | | Total | ARR | ASR Rank | ARR Rank |
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
| | RIRT | SIRT | RIST | SIST | RIRT | SIRT | RIST | SIST | RIRT | SIRT | RIST | SIST | | | | |
| Claude35-sonnet2 | 8.11% | 8.57% | 29.33% | 31.75% | 4.18% | 5.04% | 23.11% | 29.45% | 10.09% | 11.00% | 32.72% | 33.54% | 18.31% | 25.82% | 1 | 16 |
| GPT-4o | 22.86% | 36.92% | 68.49% | 72.58% | 31.00% | 41.46% | 65.27% | 77.49% | 37.75% | 69.12% | 67.39% | 80.63% | 58.01% | 6.81% | 2 | 9 |
| Gemini-1.5-pro | 40.54% | 72.86% | 60.00% | 85.71% | 50.00% | 71.70% | 72.95% | 81.39% | 32.22% | 60.33% | 55.56% | 70.57% | 63.07% | 11.27% | 3 | 11 |
| Gemini-2.0-flash | 59.46% | 80.00% | 78.67% | 93.65% | 48.61% | 65.95% | 84.87% | 85.89% | 43.47% | 67.33% | 78.40% | 84.49% | 70.71% | 5.43% | 4 | 5 |
| Qwen2.5-VL-72B | 56.76% | 89.86% | 76.00% | 92.06% | 68.26% | 85.82% | 86.16% | 88.75% | 52.13% | 80.20% | 70.37% | 85.08% | 77.85% | 1.43% | 5 | 1 |
| InternVL2-40B | 68.92% | 90.91% | 83.33% | 93.10% | 75.73% | 89.39% | 92.11% | 91.92% | 53.02% | 79.11% | 78.14% | 81.19% | 81.43% | 11.76% | 6 | 13 |
| Qwen2-VL-7B | 72.97% | 84.06% | 86.67% | 88.89% | 73.51% | 88.25% | 92.84% | 92.84% | 56.23% | 76.33% | 81.48% | 83.86% | 81.84% | 6.27% | 7 | 7 |
| InternVL2-8B | 83.56% | 98.55% | 86.11% | 98.41% | 78.53% | 89.00% | 90.68% | 90.93% | 61.39% | 79.25% | 78.23% | 79.22% | 82.76% | 11.97% | 8 | 14 |
| GLM-4V-9B | 64.86% | 92.86% | 85.33% | 95.24% | 81.40% | 89.93% | 92.02% | 89.37% | 62.20% | 83.00% | 81.79% | 79.05% | 83.42% | 5.99% | 9 | 6 |
| Qwen-VL-max | 78.87% | 93.94% | 85.71% | 95.08% | 79.45% | 88.97% | 92.79% | 94.58% | 58.39% | 84.70% | 81.19% | 87.71% | 84.77% | 3.77% | 10 | 3 |
| Qwen2-VL-72B | 67.12% | 92.86% | 92.00% | 95.24% | 81.67% | 91.59% | 92.84% | 93.21% | 59.57% | 86.00% | 81.79% | 85.40% | 85.10% | 1.87% | 11 | 2 |
| Qwen2.5-VL-7B | 86.49% | 97.10% | 82.67% | 96.72% | 88.28% | 95.87% | 92.83% | 94.07% | 76.99% | 91.64% | 81.00% | 83.86% | 88.96% | 4.73% | 12 | 4 |
| LLAVA-v1.5-7B | 89.19% | 91.43% | 90.54% | 83.87% | 90.42% | 94.96% | 89.78% | 91.36% | 86.02% | 90.67% | 85.49% | 85.40% | 89.57% | 8.54% | 13 | 10 |
| MiniCPM-V 2.6 | 93.24% | 98.57% | 90.67% | 93.65% | 92.83% | 98.08% | 90.18% | 94.67% | 83.28% | 93.33% | 80.25% | 81.96% | 90.40% | 6.43% | 14 | 8 |
| LLAVA-v1.5-13B | 94.59% | 98.57% | 93.33% | 96.77% | 90.38% | 96.84% | 94.44% | 93.40% | 78.53% | 94.00% | 85.49% | 84.08% | 90.75% | 11.39% | 15 | 12 |
| VILA-13B | 97.30% | 97.14% | 92.00% | 92.06% | 95.41% | 97.12% | 93.46% | 93.05% | 92.40% | 96.67% | 87.96% | 85.44% | 93.15% | 22.34% | 16 | 15 |
| VILA-7B | 94.59% | 98.57% | 94.67% | 95.24% | 95.02% | 96.64% | 94.26% | 94.89% | 95.14% | 96.33% | 87.35% | 88.61% | 93.97% | 32.51% | 17 | 17 |

